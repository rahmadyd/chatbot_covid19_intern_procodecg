{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292c5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’š FAISS index disimpan di : D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\n",
      "Total data: 20 chunk\n",
      "ğŸ“ File teks disimpan di : D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19_texts.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Path file\n",
    "embd_path = r\"D:/chatbot_covid19_intern_procodecg/vector_db/faiss__metadata.json\"\n",
    "faiss_path = r\"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\"\n",
    "\n",
    "# Pastikan folder tujuan ada\n",
    "os.makedirs(os.path.dirname(faiss_path), exist_ok=True)\n",
    "\n",
    "# List untuk menampung data\n",
    "text = []\n",
    "vectors = []\n",
    "\n",
    "# Membaca file embedding\n",
    "with open(embd_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        text.append(record[\"text\"])\n",
    "        vectors.append(record[\"embedding\"])\n",
    "\n",
    "# Ubah list ke numpy array\n",
    "vectors = np.array(vectors).astype(\"float32\")\n",
    "\n",
    "# Buat FAISS index (L2 distance)\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "index.add(vectors)\n",
    "\n",
    "# Simpan FAISS index ke file .index\n",
    "faiss.write_index(index, faiss_path)\n",
    "\n",
    "# Info hasil\n",
    "print(f\"ğŸ’š FAISS index disimpan di : {faiss_path}\")\n",
    "print(f\"Total data: {len(text)} chunk\")\n",
    "\n",
    "# Simpan teks referensi dalam file JSON (bukan FAISS)\n",
    "text_json_path = faiss_path.replace(\".index\", \"_texts.json\")\n",
    "with open(text_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(text, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ğŸ“ File teks disimpan di : {text_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bcf7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada file?: True\n",
      "Ukuran file (byte): 61485\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "faiss_path = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\"\n",
    "print(\"Ada file?:\", os.path.exists(faiss_path))\n",
    "print(\"Ukuran file (byte):\", os.path.getsize(faiss_path) if os.path.exists(faiss_path) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86a4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'IxF2\\x00\\x03\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x00'\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\", \"rb\") as f:\n",
    "    header = f.read(20)\n",
    "print(header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a47d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dimensi Vektor (index.d) dalam file FAISS: 768\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# --- 1. Muat (Load) Indeks FAISS ---\n",
    "# Ganti path sesuai dengan lokasi file Anda jika perlu\n",
    "index = faiss.read_index(\"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\")\n",
    "\n",
    "# --- 2. Cek Dimensi Vektor (d) ---\n",
    "dimensi_faiss = index.d\n",
    "\n",
    "print(f\"âœ… Dimensi Vektor (index.d) dalam file FAISS: {dimensi_faiss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b934f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING FAISS INDEX CREATION...\n",
      "ğŸ“ Project root: D:/chatbot_covid19_intern_procodecg\n",
      "ğŸ“ FAISS dir: D:/chatbot_covid19_intern_procodecg\\faiss\n",
      "ğŸ“ JSONL source: D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\n",
      "\n",
      "==================================================\n",
      "ğŸ“– STEP 1: Loading data from JSONL...\n",
      "ğŸ“– Loading JSONL from: D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\n",
      "âœ… Successfully loaded 20 chunks from JSONL\n",
      "\n",
      "==================================================\n",
      "ğŸ”¤ STEP 2: Creating embeddings...\n",
      "ğŸ”„ Loading embedding model...\n",
      "ğŸ”„ Creating embeddings...\n",
      "   Processed 20/20 chunks...\n",
      "âœ… Created embeddings: (20, 768)\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š STEP 3: Creating FAISS index...\n",
      "ğŸ”„ Creating FAISS index...\n",
      "âœ… FAISS index created with 20 vectors\n",
      "\n",
      "==================================================\n",
      "ğŸ’¾ STEP 4: Saving index and data...\n",
      "ğŸ’¾ Saving index and data...\n",
      "âœ… Saved chunks to: D:/chatbot_covid19_intern_procodecg\\data\\chunks.json\n",
      "âœ… Saved texts to: D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19_texts.json\n",
      "âœ… Saved FAISS index to: D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19.index\n",
      "\n",
      "==================================================\n",
      "âœ… STEP 5: Verification...\n",
      "\n",
      "ğŸ” Verifying index...\n",
      "   FAISS Index: âœ… D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19.index (61485 bytes)\n",
      "   Texts File: âœ… D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19_texts.json (15763 bytes)\n",
      "   Chunks File: âœ… D:/chatbot_covid19_intern_procodecg\\data\\chunks.json (17873 bytes)\n",
      "   Index vectors: âœ… 20\n",
      "   Text chunks: âœ… 20\n",
      "\n",
      "==================================================\n",
      "ğŸ” STEP 6: Testing search...\n",
      "\n",
      "ğŸ” Testing search with 6 queries...\n",
      "\n",
      "ğŸ“ Query: 'Siapa orang pertama yang divaksin di Indonesia?'\n",
      "   1. Score: 0.684 [âœ… GOOD]\n",
      "       Text: 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2...\n",
      "   2. Score: 0.609 [âœ… GOOD]\n",
      "       Text: 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksi...\n",
      "   3. Score: 0.492 [âœ… GOOD]\n",
      "       Text: 12. Sistem Kesehatan Nasional\n",
      "Sistem kesehatan Indonesia menghadapi tekanan besar selama pandemi. Pa...\n",
      "\n",
      "ğŸ“ Query: 'Apa vaksin kedua yang digunakan?'\n",
      "   1. Score: 0.651 [âœ… GOOD]\n",
      "       Text: 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2...\n",
      "   2. Score: 0.636 [âœ… GOOD]\n",
      "       Text: 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksi...\n",
      "   3. Score: 0.477 [âœ… GOOD]\n",
      "       Text: 5. Edukasi Publik dan Penanggulangan Hoaks\n",
      "Pandemi COVID-19 juga diiringi oleh fenomena infodemi, ya...\n",
      "\n",
      "ğŸ“ Query: 'Jenis vaksin COVID-19 di Indonesia'\n",
      "   1. Score: 0.754 [âœ… GOOD]\n",
      "       Text: 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbarui data kasus COVID-1...\n",
      "   2. Score: 0.657 [âœ… GOOD]\n",
      "       Text: 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2...\n",
      "   3. Score: 0.657 [âœ… GOOD]\n",
      "       Text: 8. Aplikasi dan Teknologi Pendukung\n",
      "Digitalisasi menjadi bagian penting dari strategi penanganan COV...\n",
      "\n",
      "ğŸ“ Query: 'Presiden Jokowi vaksin'\n",
      "   1. Score: 0.728 [âœ… GOOD]\n",
      "       Text: 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksi...\n",
      "   2. Score: 0.704 [âœ… GOOD]\n",
      "       Text: 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2...\n",
      "   3. Score: 0.461 [âœ… GOOD]\n",
      "       Text: 18. Inovasi dan Riset Dalam Negeri\n",
      "Pandemi mendorong kemajuan riset dan inovasi di Indonesia. Lembag...\n",
      "\n",
      "ğŸ“ Query: 'Gejala COVID-19'\n",
      "   1. Score: 0.676 [âœ… GOOD]\n",
      "       Text: 1. Informasi Medis dan Kesehatan Publik\n",
      "COVID-19 adalah penyakit menular yang disebabkan oleh virus ...\n",
      "   2. Score: 0.506 [âœ… GOOD]\n",
      "       Text: 9. Tanya Jawab Cepat dan Edukasi Umum\n",
      "Chatbot COVID-19 biasanya juga menyediakan jawaban cepat untuk...\n",
      "   3. Score: 0.495 [âœ… GOOD]\n",
      "       Text: 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbarui data kasus COVID-1...\n",
      "\n",
      "ğŸ“ Query: 'Aplikasi PeduliLindungi'\n",
      "   1. Score: 0.616 [âœ… GOOD]\n",
      "       Text: 8. Aplikasi dan Teknologi Pendukung\n",
      "Digitalisasi menjadi bagian penting dari strategi penanganan COV...\n",
      "   2. Score: 0.524 [âœ… GOOD]\n",
      "       Text: 4. Panduan Aktivitas dan Mobilitas\n",
      "Untuk menekan penyebaran virus, pemerintah memberlakukan berbagai...\n",
      "   3. Score: 0.478 [âœ… GOOD]\n",
      "       Text: 19. Reformasi dan Ketahanan Nasional\n",
      "Setelah pandemi mereda, pemerintah menyusun rencana reformasi s...\n",
      "\n",
      "ğŸ“Š SEARCH TEST SUMMARY:\n",
      "   Good matches: 6/6\n",
      "   Success rate: 100.0%\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ FAISS INDEX CREATION COMPLETED!\n",
      "ğŸ“Š Total chunks: 20\n",
      "ğŸ“Š Embedding dimension: 768\n",
      "ğŸ“Š FAISS index size: 20 vectors\n",
      "â±ï¸ Total time: 14.13 seconds\n",
      "\n",
      "âœ… SUCCESS: Index created and tested successfully!\n",
      "\n",
      "ğŸ“‹ NEXT STEPS:\n",
      "1. Update your retriever.py to use the new index\n",
      "2. Clear Streamlit cache\n",
      "3. Test your chatbot!\n",
      "\n",
      "ğŸ‰ Script completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# create_faiss_index.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "def setup_paths():\n",
    "    \"\"\"Setup project paths\"\"\"\n",
    "    project_root = r\"D:/chatbot_covid19_intern_procodecg\"\n",
    "    faiss_dir = os.path.join(project_root, \"faiss\")\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "    \n",
    "    # Buat folder jika belum ada\n",
    "    os.makedirs(faiss_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'project_root': project_root,\n",
    "        'faiss_dir': faiss_dir,\n",
    "        'data_dir': data_dir,\n",
    "        'index_path': os.path.join(faiss_dir, \"faiss_textcovid19.index\"),\n",
    "        'texts_path': os.path.join(faiss_dir, \"faiss_textcovid19_texts.json\"),\n",
    "        'chunks_path': os.path.join(data_dir, \"chunks.json\"),\n",
    "        'jsonl_path': r\"D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\"\n",
    "    }\n",
    "\n",
    "def load_and_process_jsonl(jsonl_path):\n",
    "    \"\"\"Load dan process file JSONL\"\"\"\n",
    "    print(f\"ğŸ“– Loading JSONL from: {jsonl_path}\")\n",
    "    \n",
    "    if not os.path.exists(jsonl_path):\n",
    "        print(f\"âŒ JSONL file not found: {jsonl_path}\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        \n",
    "                        # Extract text dari berbagai kemungkinan field\n",
    "                        text = \"\"\n",
    "                        if 'text' in data:\n",
    "                            text = data['text']\n",
    "                        elif 'content' in data:\n",
    "                            text = data['content'] \n",
    "                        elif 'chunk' in data:\n",
    "                            text = data['chunk']\n",
    "                        elif 'page_content' in data:\n",
    "                            text = data['page_content']\n",
    "                        else:\n",
    "                            # Coba cari field yang mengandung text\n",
    "                            for key, value in data.items():\n",
    "                                if isinstance(value, str) and len(value) > 20:\n",
    "                                    text = value\n",
    "                                    break\n",
    "                            if not text:\n",
    "                                text = str(data)\n",
    "                        \n",
    "                        # Clean text\n",
    "                        if text:\n",
    "                            text = text.strip()\n",
    "                            if len(text) > 10:  # Minimal 10 karakter\n",
    "                                chunks.append({\n",
    "                                    'id': len(chunks),\n",
    "                                    'text': text,\n",
    "                                    'source': data.get('source', 'unknown'),\n",
    "                                    'metadata': data.get('metadata', {})\n",
    "                                })\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        if (i + 1) % 100 == 0:\n",
    "                            print(f\"   Processed {i + 1} lines...\")\n",
    "                            \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"âš ï¸ JSON error line {i}: {e}\")\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Error processing line {i}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded {len(chunks)} chunks from JSONL\")\n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading JSONL: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    \"\"\"Create embeddings untuk semua chunks\"\"\"\n",
    "    print(\"ğŸ”„ Loading embedding model...\")\n",
    "    \n",
    "    try:\n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        \n",
    "        print(\"ğŸ”„ Creating embeddings...\")\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Create embeddings dengan batch processing\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "            \n",
    "            # Progress\n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f\"   Processed {min(i + batch_size, len(texts))}/{len(texts)} chunks...\")\n",
    "        \n",
    "        # Combine all embeddings\n",
    "        embeddings = np.vstack(all_embeddings).astype('float32')\n",
    "        \n",
    "        print(f\"âœ… Created embeddings: {embeddings.shape}\")\n",
    "        return embeddings, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating embeddings: {e}\")\n",
    "        raise e\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create FAISS index dari embeddings\"\"\"\n",
    "    print(\"ğŸ”„ Creating FAISS index...\")\n",
    "    \n",
    "    try:\n",
    "        dimension = embeddings.shape[1]\n",
    "        \n",
    "        # Gunakan IndexFlatIP untuk cosine similarity\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        \n",
    "        # Normalize vectors untuk cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Add ke index\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        print(f\"âœ… FAISS index created with {index.ntotal} vectors\")\n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating FAISS index: {e}\")\n",
    "        raise e\n",
    "\n",
    "def save_index_data(chunks, index, paths):\n",
    "    \"\"\"Save index dan data terkait\"\"\"\n",
    "    print(\"ğŸ’¾ Saving index and data...\")\n",
    "    \n",
    "    try:\n",
    "        # Save chunks dengan metadata\n",
    "        with open(paths['chunks_path'], 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… Saved chunks to: {paths['chunks_path']}\")\n",
    "        \n",
    "        # Save texts untuk retriever (hanya teks)\n",
    "        texts_for_retriever = [chunk['text'] for chunk in chunks]\n",
    "        with open(paths['texts_path'], 'w', encoding='utf-8') as f:\n",
    "            json.dump(texts_for_retriever, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… Saved texts to: {paths['texts_path']}\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(index, paths['index_path'])\n",
    "        print(f\"âœ… Saved FAISS index to: {paths['index_path']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving data: {e}\")\n",
    "        raise e\n",
    "\n",
    "def verify_index(paths):\n",
    "    \"\"\"Verify bahwa index berhasil dibuat\"\"\"\n",
    "    print(\"\\nğŸ” Verifying index...\")\n",
    "    \n",
    "    try:\n",
    "        # Check files exist\n",
    "        required_files = {\n",
    "            'FAISS Index': paths['index_path'],\n",
    "            'Texts File': paths['texts_path'],\n",
    "            'Chunks File': paths['chunks_path']\n",
    "        }\n",
    "        \n",
    "        for name, path in required_files.items():\n",
    "            exists = os.path.exists(path)\n",
    "            size = os.path.getsize(path) if exists else 0\n",
    "            print(f\"   {name}: {'âœ…' if exists else 'âŒ'} {path} ({size} bytes)\")\n",
    "        \n",
    "        # Test load index\n",
    "        index = faiss.read_index(paths['index_path'])\n",
    "        print(f\"   Index vectors: âœ… {index.ntotal}\")\n",
    "        \n",
    "        # Test load texts\n",
    "        with open(paths['texts_path'], 'r', encoding='utf-8') as f:\n",
    "            texts = json.load(f)\n",
    "        print(f\"   Text chunks: âœ… {len(texts)}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_search(paths, test_queries=None):\n",
    "    \"\"\"Test search functionality\"\"\"\n",
    "    if test_queries is None:\n",
    "        test_queries = [\n",
    "            \"Siapa orang pertama yang divaksin di Indonesia?\",\n",
    "            \"Apa vaksin kedua yang digunakan?\",\n",
    "            \"Jenis vaksin COVID-19 di Indonesia\",\n",
    "            \"Presiden Jokowi vaksin\",\n",
    "            \"Gejala COVID-19\",\n",
    "            \"Aplikasi PeduliLindungi\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\nğŸ” Testing search with {len(test_queries)} queries...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        \n",
    "        # Load index dan texts\n",
    "        index = faiss.read_index(paths['index_path'])\n",
    "        with open(paths['texts_path'], 'r', encoding='utf-8') as f:\n",
    "            texts = json.load(f)\n",
    "        \n",
    "        results_summary = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nğŸ“ Query: '{query}'\")\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            \n",
    "            # Search\n",
    "            scores, indices = index.search(query_embedding, 3)\n",
    "            \n",
    "            found_good = False\n",
    "            for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "                if 0 <= idx < len(texts):\n",
    "                    quality = \"âœ… GOOD\" if score > 0.3 else \"âš ï¸ WEAK\" if score > 0.15 else \"âŒ POOR\"\n",
    "                    print(f\"   {i+1}. Score: {score:.3f} [{quality}]\")\n",
    "                    \n",
    "                    if score > 0.3:\n",
    "                        found_good = True\n",
    "                        text_preview = texts[idx][:100] + \"...\" if len(texts[idx]) > 100 else texts[idx]\n",
    "                        print(f\"       Text: {text_preview}\")\n",
    "            \n",
    "            results_summary.append({\n",
    "                'query': query,\n",
    "                'found_good_match': found_good,\n",
    "                'top_score': scores[0][0] if len(scores[0]) > 0 else 0\n",
    "            })\n",
    "            \n",
    "            if not found_good:\n",
    "                print(\"   âŒ NO GOOD MATCHES FOUND!\")\n",
    "        \n",
    "        # Summary\n",
    "        good_matches = sum(1 for r in results_summary if r['found_good_match'])\n",
    "        print(f\"\\nğŸ“Š SEARCH TEST SUMMARY:\")\n",
    "        print(f\"   Good matches: {good_matches}/{len(test_queries)}\")\n",
    "        print(f\"   Success rate: {(good_matches/len(test_queries))*100:.1f}%\")\n",
    "        \n",
    "        return good_matches > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Search test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function untuk create FAISS index\"\"\"\n",
    "    print(\"ğŸš€ STARTING FAISS INDEX CREATION...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Setup paths\n",
    "        paths = setup_paths()\n",
    "        print(f\"ğŸ“ Project root: {paths['project_root']}\")\n",
    "        print(f\"ğŸ“ FAISS dir: {paths['faiss_dir']}\")\n",
    "        print(f\"ğŸ“ JSONL source: {paths['jsonl_path']}\")\n",
    "        \n",
    "        # Step 1: Load data dari JSONL\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“– STEP 1: Loading data from JSONL...\")\n",
    "        chunks = load_and_process_jsonl(paths['jsonl_path'])\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"âŒ No chunks loaded! Exiting.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 2: Create embeddings\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ”¤ STEP 2: Creating embeddings...\")\n",
    "        embeddings, model = create_embeddings(chunks)\n",
    "        \n",
    "        # Step 3: Create FAISS index\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“Š STEP 3: Creating FAISS index...\")\n",
    "        index = create_faiss_index(embeddings)\n",
    "        \n",
    "        # Step 4: Save data\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ’¾ STEP 4: Saving index and data...\")\n",
    "        save_index_data(chunks, index, paths)\n",
    "        \n",
    "        # Step 5: Verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"âœ… STEP 5: Verification...\")\n",
    "        verification_ok = verify_index(paths)\n",
    "        \n",
    "        # Step 6: Test search\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ” STEP 6: Testing search...\")\n",
    "        search_ok = test_search(paths)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ‰ FAISS INDEX CREATION COMPLETED!\")\n",
    "        print(f\"ğŸ“Š Total chunks: {len(chunks)}\")\n",
    "        print(f\"ğŸ“Š Embedding dimension: {embeddings.shape[1]}\")\n",
    "        print(f\"ğŸ“Š FAISS index size: {index.ntotal} vectors\")\n",
    "        print(f\"â±ï¸ Total time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        if verification_ok and search_ok:\n",
    "            print(\"\\nâœ… SUCCESS: Index created and tested successfully!\")\n",
    "            print(\"\\nğŸ“‹ NEXT STEPS:\")\n",
    "            print(\"1. Update your retriever.py to use the new index\")\n",
    "            print(\"2. Clear Streamlit cache\")\n",
    "            print(\"3. Test your chatbot!\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ WARNING: Index created but some tests failed\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ Script completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nğŸ’¥ Script failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
