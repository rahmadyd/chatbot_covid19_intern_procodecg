{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ecad8f",
   "metadata": {},
   "source": [
    "Merge JSONL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05851a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semua chunk telah digabung di: D:/chatbot_covid19_intern_procodecg/data/processed/dataset_covid__merge.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files = [\n",
    "    r\"D:/chatbot_covid19_intern_procodecg/data/processed/covid_19_to_narative.jsonl\",\n",
    "    r\"D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\"\n",
    "]\n",
    "\n",
    "merge_file = r\"D:/chatbot_covid19_intern_procodecg/data/processed/dataset_covid__merge.jsonl\"\n",
    "\n",
    "with open(merge_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for path in files :\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "                \n",
    "print(f\"semua chunk telah digabung di: {merge_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdfe33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Memuat model SentenceTransformer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28f1134f0024e0ba2edb6e26aab0810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e6e7025cad42a8ab36b3da69442870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063facaaf6c441ec82fa60b7177e9fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef361532884e27bb7372678192c86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac12be0d07ee4111b8567faa9c0e08d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot_covid19_intern_procodecg\\.venvr\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk 1 selesai di-embed\n",
      "âœ… Chunk 2 selesai di-embed\n",
      "âœ… Chunk 3 selesai di-embed\n",
      "âœ… Chunk 4 selesai di-embed\n",
      "âœ… Chunk 5 selesai di-embed\n",
      "âœ… Chunk 6 selesai di-embed\n",
      "âœ… Chunk 7 selesai di-embed\n",
      "âœ… Chunk 8 selesai di-embed\n",
      "âœ… Chunk 9 selesai di-embed\n",
      "âœ… Chunk 10 selesai di-embed\n",
      "âœ… Chunk 11 selesai di-embed\n",
      "âœ… Chunk 12 selesai di-embed\n",
      "âœ… Chunk 13 selesai di-embed\n",
      "âœ… Chunk 14 selesai di-embed\n",
      "âœ… Chunk 15 selesai di-embed\n",
      "âœ… Chunk 16 selesai di-embed\n",
      "âœ… Chunk 17 selesai di-embed\n",
      "âœ… Chunk 18 selesai di-embed\n",
      "âœ… Chunk 19 selesai di-embed\n",
      "âœ… Chunk 20 selesai di-embed\n",
      "ğŸ’¾ Semua embedding tersimpan di: D:/chatbot_covid19_intern_procodecg/vector_db/faiss__bahasa_metadata.json\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "chunks_path = r\"D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\"\n",
    "embedding_path = r\"D:/chatbot_covid19_intern_procodecg/vector_db/faiss__bahasa_metadata.json\"\n",
    "\n",
    "# 1ï¸âƒ£ Load model lokal untuk embedding\n",
    "print(\"ğŸ§  Memuat model SentenceTransformer ...\")\n",
    "model = SentenceTransformer(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# 2ï¸âƒ£ Baca dan buat embedding per chunk\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        text = data[\"text\"]\n",
    "\n",
    "        emb = model.encode(text).tolist()\n",
    "        data[\"embedding\"] = emb\n",
    "\n",
    "        embeddings.append(data)\n",
    "        print(f\"âœ… Chunk {i+1} selesai di-embed\")\n",
    "\n",
    "# 3ï¸âƒ£ Simpan hasilnya ke JSONL\n",
    "with open(embedding_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in embeddings:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"ğŸ’¾ Semua embedding tersimpan di: {embedding_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d232815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class COVID19EmbeddingGenerator:\n",
    "    def __init__(self, model_name=\"thenlper/gte-base\"):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator dengan model pilihan\n",
    "        \n",
    "        Model options:\n",
    "        - \"thenlper/gte-base\" (Recommended)\n",
    "        - \"all-mpnet-base-v2\"\n",
    "        - \"intfloat/multilingual-e5-base\" \n",
    "        - \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        - \"all-MiniLM-L12-v2\" (Lightweight)\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ§  Loading model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def load_chunks(self, chunks_path):\n",
    "        \"\"\"Load chunks dari JSONL file\"\"\"\n",
    "        chunks = []\n",
    "        with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                chunks.append(data)\n",
    "        print(f\"ğŸ“– Loaded {len(chunks)} chunks from {chunks_path}\")\n",
    "        return chunks\n",
    "    \n",
    "    def create_embeddings(self, chunks_path, output_path, batch_size=16):\n",
    "        \"\"\"Buat embeddings untuk semua chunks\"\"\"\n",
    "        chunks = self.load_chunks(chunks_path)\n",
    "        \n",
    "        print(\"ğŸ” Creating embeddings...\")\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing batches\"):\n",
    "            batch_chunks = chunks[i:i + batch_size]\n",
    "            batch_texts = [chunk[\"text\"] for chunk in batch_chunks]\n",
    "            \n",
    "            # Buat embeddings dengan normalisasi\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch_texts,\n",
    "                normalize_embeddings=True,  # â­ PENTING untuk reduce halusinasi\n",
    "                show_progress_bar=False,\n",
    "                convert_to_tensor=False\n",
    "            )\n",
    "            \n",
    "            # Tambah metadata\n",
    "            for j, chunk in enumerate(batch_chunks):\n",
    "                chunk[\"embedding\"] = batch_embeddings[j].tolist()\n",
    "                chunk[\"embedding_model\"] = self.model_name\n",
    "                chunk[\"chunk_id\"] = f\"chunk_{i+j}\"\n",
    "                chunk[\"embedding_version\"] = \"v2_optimized\"\n",
    "            \n",
    "            all_embeddings.extend(batch_chunks)\n",
    "        \n",
    "        # Simpan hasil\n",
    "        self._save_embeddings(all_embeddings, output_path)\n",
    "        return all_embeddings\n",
    "    \n",
    "    def _save_embeddings(self, embeddings, output_path):\n",
    "        \"\"\"Save embeddings ke file JSONL\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in embeddings:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"ğŸ’¾ Embeddings saved to: {output_path}\")\n",
    "        print(f\"ğŸ“Š Total embeddings: {len(embeddings)}\")\n",
    "\n",
    "def main():\n",
    "    # ğŸ”§ CONFIGURATION - GANTI DI SINI UNTUK COBA MODEL BERBEDA\n",
    "    MODEL_CONFIGS = {\n",
    "        \"gte-base\": \"thenlper/gte-base\",  # Recommended\n",
    "        \"mpnet-base\": \"all-mpnet-base-v2\",\n",
    "        \"multilingual-e5\": \"intfloat/multilingual-e5-base\", \n",
    "        \"multilingual-mpnet\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        \"minilm\": \"all-MiniLM-L12-v2\"  # Lightweight\n",
    "    }\n",
    "    \n",
    "    # Pilih model yang mau dicoba\n",
    "    SELECTED_MODEL = \"gte-base\"  # ğŸ”„ GANTI INI UNTUK COBA MODEL LAIN\n",
    "    \n",
    "    # Path files\n",
    "    CHUNKS_PATH = r\"D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\"\n",
    "    OUTPUT_PATH = r\"D:/chatbot_covid19_intern_procodecg/vector_db/embeddings_optimized.jsonl\"\n",
    "    \n",
    "    print(f\"ğŸ¯ Using model: {SELECTED_MODEL} -> {MODEL_CONFIGS[SELECTED_MODEL]}\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = COVID19EmbeddingGenerator(MODEL_CONFIGS[SELECTED_MODEL])\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = generator.create_embeddings(\n",
    "        chunks_path=CHUNKS_PATH,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        batch_size=8  # Adjust berdasarkan GPU memory\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Embedding generation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df0d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING EMBEDDING CREATION FROM JSONL...\n",
      "ğŸ“ Project root: D:/chatbot_covid19_intern_procodecg\n",
      "ğŸ“ FAISS dir: D:/chatbot_covid19_intern_procodecg\\faiss\n",
      "ğŸ“ JSONL source: D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\n",
      "\n",
      "ğŸ“– Step 1: Loading chunks from JSONL...\n",
      "ğŸ“– Loading chunks from JSONL: D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\n",
      "âœ… Loaded 20 chunks from JSONL\n",
      "\n",
      "ğŸ“ Sample chunks from JSONL:\n",
      "   1. 1. Informasi Medis dan Kesehatan Publik\n",
      "COVID-19 adalah penyakit menular yang disebabkan oleh virus ...\n",
      "   2. 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2...\n",
      "   3. 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbarui data kasus COVID-1...\n",
      "   4. 4. Panduan Aktivitas dan Mobilitas\n",
      "Untuk menekan penyebaran virus, pemerintah memberlakukan berbagai...\n",
      "   5. 5. Edukasi Publik dan Penanggulangan Hoaks\n",
      "Pandemi COVID-19 juga diiringi oleh fenomena infodemi, ya...\n",
      "\n",
      "ğŸ”¤ Step 2: Creating embeddings...\n",
      "ğŸ”„ Loading embedding model...\n",
      "ğŸ”„ Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1256c12105264908aa1f43500a293717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created embeddings: (20, 768)\n",
      "\n",
      "ğŸ“Š Step 3: Creating FAISS index...\n",
      "ğŸ”„ Creating FAISS index...\n",
      "âœ… FAISS index created: 20 vectors\n",
      "\n",
      "ğŸ’¾ Step 4: Saving data...\n",
      "ğŸ”„ Saving data...\n",
      "âœ… Saved 20 chunks to: D:/chatbot_covid19_intern_procodecg\\data\\chunks.json\n",
      "âœ… Saved 20 texts to: D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19_texts.json\n",
      "âœ… Saved index to: D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19.index\n",
      "\n",
      "âœ… Step 5: Verification...\n",
      "\n",
      "ğŸ” Verifying saved data...\n",
      "   JSONL source: âœ… EXISTS - D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\n",
      "   Chunks file: âœ… EXISTS - D:/chatbot_covid19_intern_procodecg\\data\\chunks.json\n",
      "   Texts file: âœ… EXISTS - D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19_texts.json\n",
      "   Index file: âœ… EXISTS - D:/chatbot_covid19_intern_procodecg\\faiss\\faiss_textcovid19.index\n",
      "   Chunks count: âœ… 20\n",
      "\n",
      "ğŸ“ Sample saved chunks:\n",
      "   1. 1. Informasi Medis dan Kesehatan Publik\n",
      "COVID-19 adalah penyakit menular yang di...\n",
      "   2. 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nas...\n",
      "   3. 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbaru...\n",
      "   FAISS vectors: âœ… 20\n",
      "\n",
      "ğŸ” Step 6: Testing search capability...\n",
      "\n",
      "ğŸ” Testing search capability...\n",
      "ğŸ§ª Testing 6 queries...\n",
      "\n",
      "ğŸ“ Query: 'Siapa orang pertama yang divaksin di Indonesia?'\n",
      "   âœ… 1. Score: 0.684 - 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2021 sebagai upaya me...\n",
      "   âœ… 2. Score: 0.609 - 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksinasi nasional. Presi...\n",
      "   âœ… 3. Score: 0.492 - 12. Sistem Kesehatan Nasional\n",
      "Sistem kesehatan Indonesia menghadapi tekanan besar selama pandemi. Pada gelombang pertama...\n",
      "\n",
      "ğŸ“ Query: 'Apa vaksin kedua yang digunakan?'\n",
      "   âœ… 1. Score: 0.651 - 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2021 sebagai upaya me...\n",
      "   âœ… 2. Score: 0.636 - 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksinasi nasional. Presi...\n",
      "   âœ… 3. Score: 0.477 - 5. Edukasi Publik dan Penanggulangan Hoaks\n",
      "Pandemi COVID-19 juga diiringi oleh fenomena infodemi, yaitu penyebaran infor...\n",
      "\n",
      "ğŸ“ Query: 'Jenis vaksin COVID-19 di Indonesia'\n",
      "   âœ… 1. Score: 0.754 - 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbarui data kasus COVID-19 melalui situs covi...\n",
      "   âœ… 2. Score: 0.657 - 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2021 sebagai upaya me...\n",
      "   âœ… 3. Score: 0.657 - 8. Aplikasi dan Teknologi Pendukung\n",
      "Digitalisasi menjadi bagian penting dari strategi penanganan COVID-19 di Indonesia. ...\n",
      "\n",
      "ğŸ“ Query: 'Presiden Jokowi vaksin'\n",
      "   âœ… 1. Score: 0.728 - 13. Program Vaksinasi Nasional\n",
      "Awal 2021 menjadi titik balik penting dengan dimulainya program vaksinasi nasional. Presi...\n",
      "   âœ… 2. Score: 0.704 - 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2021 sebagai upaya me...\n",
      "   âœ… 3. Score: 0.461 - 18. Inovasi dan Riset Dalam Negeri\n",
      "Pandemi mendorong kemajuan riset dan inovasi di Indonesia. Lembaga seperti BRIN, Bio ...\n",
      "\n",
      "ğŸ“ Query: 'Gejala COVID-19'\n",
      "   âœ… 1. Score: 0.676 - 1. Informasi Medis dan Kesehatan Publik\n",
      "COVID-19 adalah penyakit menular yang disebabkan oleh virus SARS-CoV-2. Penulara...\n",
      "   âœ… 2. Score: 0.506 - 9. Tanya Jawab Cepat dan Edukasi Umum\n",
      "Chatbot COVID-19 biasanya juga menyediakan jawaban cepat untuk pertanyaan yang ser...\n",
      "   âœ… 3. Score: 0.495 - 3. Statistik dan Data Nasional\n",
      "Selama pandemi, Indonesia secara rutin memperbarui data kasus COVID-19 melalui situs covi...\n",
      "\n",
      "ğŸ“ Query: 'Aplikasi PeduliLindungi'\n",
      "   âœ… 1. Score: 0.616 - 8. Aplikasi dan Teknologi Pendukung\n",
      "Digitalisasi menjadi bagian penting dari strategi penanganan COVID-19 di Indonesia. ...\n",
      "   âœ… 2. Score: 0.524 - 4. Panduan Aktivitas dan Mobilitas\n",
      "Untuk menekan penyebaran virus, pemerintah memberlakukan berbagai kebijakan pembatasa...\n",
      "   âœ… 3. Score: 0.478 - 19. Reformasi dan Ketahanan Nasional\n",
      "Setelah pandemi mereda, pemerintah menyusun rencana reformasi sistem kesehatan jang...\n",
      "\n",
      "ğŸ‰ EMBEDDING CREATION COMPLETED!\n",
      "ğŸ“Š Total chunks: 20\n",
      "ğŸ“Š Embedding dimension: 768\n",
      "ğŸ“Š FAISS index size: 20 vectors\n",
      "\n",
      "ğŸ“‹ NEXT STEPS:\n",
      "1. Run your Streamlit app\n",
      "2. Clear cache di halaman 'ğŸ§¹ Cache'\n",
      "3. Test dengan: 'Siapa orang pertama yang divaksin?'\n",
      "4. Test dengan: 'Apa vaksin kedua yang digunakan?'\n"
     ]
    }
   ],
   "source": [
    "# create_embeddings.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "def setup_paths():\n",
    "    \"\"\"Setup project paths - FIXED untuk Jupyter/Colab\"\"\"\n",
    "    # Untuk Jupyter/Colab, tentukan path manual\n",
    "    project_root = r\"D:/chatbot_covid19_intern_procodecg\"\n",
    "    faiss_dir = os.path.join(project_root, \"faiss\")\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "    \n",
    "    os.makedirs(faiss_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'project_root': project_root,\n",
    "        'faiss_dir': faiss_dir,\n",
    "        'data_dir': data_dir,\n",
    "        'index_path': os.path.join(faiss_dir, \"faiss_textcovid19.index\"),\n",
    "        'texts_path': os.path.join(faiss_dir, \"faiss_textcovid19_texts.json\"),\n",
    "        'chunks_path': os.path.join(data_dir, \"chunks.json\"),\n",
    "        'jsonl_path': r\"D:/chatbot_covid19_intern_procodecg/data/processed/knowladge_covid19_indonesia.jsonl\"\n",
    "    }\n",
    "\n",
    "def load_chunks_from_jsonl(jsonl_path):\n",
    "    \"\"\"Load chunks dari file JSONL\"\"\"\n",
    "    print(f\"ğŸ“– Loading chunks from JSONL: {jsonl_path}\")\n",
    "    \n",
    "    if not os.path.exists(jsonl_path):\n",
    "        print(f\"âŒ JSONL file not found: {jsonl_path}\")\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        # Extract text dari berbagai kemungkinan field names\n",
    "                        text = \"\"\n",
    "                        if 'text' in data:\n",
    "                            text = data['text']\n",
    "                        elif 'content' in data:\n",
    "                            text = data['content']\n",
    "                        elif 'chunk' in data:\n",
    "                            text = data['chunk']\n",
    "                        elif 'page_content' in data:\n",
    "                            text = data['page_content']\n",
    "                        else:\n",
    "                            # Jika tidak ada field yang dikenal, gunakan seluruh data sebagai string\n",
    "                            text = str(data)\n",
    "                        \n",
    "                        if text and len(text.strip()) > 10:  # Minimal 10 karakter\n",
    "                            chunks.append({\n",
    "                                'id': len(chunks),\n",
    "                                'text': text.strip(),\n",
    "                                'source': data.get('source', 'unknown'),\n",
    "                                'metadata': data.get('metadata', {})\n",
    "                            })\n",
    "                            \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"âš ï¸ JSON decode error on line {i}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(chunks)} chunks from JSONL\")\n",
    "        \n",
    "        # Show sample chunks\n",
    "        print(f\"\\nğŸ“ Sample chunks from JSONL:\")\n",
    "        for i in range(min(5, len(chunks))):\n",
    "            print(f\"   {i+1}. {chunks[i]['text'][:100]}...\")\n",
    "            \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading JSONL: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    \"\"\"Create embeddings untuk semua chunks\"\"\"\n",
    "    print(\"ğŸ”„ Loading embedding model...\")\n",
    "    \n",
    "    # Gunakan model yang sama dengan retriever\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    \n",
    "    print(\"ğŸ”„ Creating embeddings...\")\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    print(f\"âœ… Created embeddings: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create FAISS index dari embeddings\"\"\"\n",
    "    print(\"ğŸ”„ Creating FAISS index...\")\n",
    "    \n",
    "    # Dimensionality\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product untuk cosine similarity\n",
    "    \n",
    "    # Normalize vectors untuk cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Add ke index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"âœ… FAISS index created: {index.ntotal} vectors\")\n",
    "    return index\n",
    "\n",
    "def save_data(chunks, index, paths):\n",
    "    \"\"\"Save semua data ke file\"\"\"\n",
    "    print(\"ğŸ”„ Saving data...\")\n",
    "    \n",
    "    # Save chunks dengan metadata lengkap\n",
    "    with open(paths['chunks_path'], 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save texts untuk retriever (hanya teks saja)\n",
    "    texts_for_retriever = [chunk['text'] for chunk in chunks]\n",
    "    with open(paths['texts_path'], 'w', encoding='utf-8') as f:\n",
    "        json.dump(texts_for_retriever, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, paths['index_path'])\n",
    "    \n",
    "    print(f\"âœ… Saved {len(chunks)} chunks to: {paths['chunks_path']}\")\n",
    "    print(f\"âœ… Saved {len(texts_for_retriever)} texts to: {paths['texts_path']}\")\n",
    "    print(f\"âœ… Saved index to: {paths['index_path']}\")\n",
    "\n",
    "def verify_data(paths):\n",
    "    \"\"\"Verify bahwa data berhasil disimpan\"\"\"\n",
    "    print(\"\\nğŸ” Verifying saved data...\")\n",
    "    \n",
    "    # Check files exist\n",
    "    for file_type, path in {\n",
    "        'JSONL source': paths['jsonl_path'],\n",
    "        'Chunks file': paths['chunks_path'],\n",
    "        'Texts file': paths['texts_path'],\n",
    "        'Index file': paths['index_path']\n",
    "    }.items():\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"   {file_type}: {'âœ… EXISTS' if exists else 'âŒ MISSING'} - {path}\")\n",
    "    \n",
    "    # Load and check chunks\n",
    "    try:\n",
    "        with open(paths['chunks_path'], 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"   Chunks count: âœ… {len(chunks)}\")\n",
    "        \n",
    "        # Show sample chunks\n",
    "        print(f\"\\nğŸ“ Sample saved chunks:\")\n",
    "        for i in range(min(3, len(chunks))):\n",
    "            print(f\"   {i+1}. {chunks[i]['text'][:80]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error loading chunks: {e}\")\n",
    "    \n",
    "    # Load and check index\n",
    "    try:\n",
    "        index = faiss.read_index(paths['index_path'])\n",
    "        print(f\"   FAISS vectors: âœ… {index.ntotal}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error loading index: {e}\")\n",
    "\n",
    "def test_search_capability(paths):\n",
    "    \"\"\"Test kemampuan search setelah membuat embeddings\"\"\"\n",
    "    print(\"\\nğŸ” Testing search capability...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        \n",
    "        # Load index dan texts\n",
    "        index = faiss.read_index(paths['index_path'])\n",
    "        with open(paths['texts_path'], 'r', encoding='utf-8') as f:\n",
    "            texts = json.load(f)\n",
    "        \n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"Siapa orang pertama yang divaksin di Indonesia?\",\n",
    "            \"Apa vaksin kedua yang digunakan?\",\n",
    "            \"Jenis vaksin COVID-19 di Indonesia\",\n",
    "            \"Presiden Jokowi vaksin\",\n",
    "            \"Gejala COVID-19\",\n",
    "            \"Aplikasi PeduliLindungi\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"ğŸ§ª Testing {len(test_queries)} queries...\")\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nğŸ“ Query: '{query}'\")\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            \n",
    "            # Search\n",
    "            scores, indices = index.search(query_embedding, 3)\n",
    "            \n",
    "            found_good_match = False\n",
    "            for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "                if 0 <= idx < len(texts) and score > 0.2:  # Threshold reasonable\n",
    "                    text_preview = texts[idx][:120] + \"...\" if len(texts[idx]) > 120 else texts[idx]\n",
    "                    print(f\"   âœ… {i+1}. Score: {score:.3f} - {text_preview}\")\n",
    "                    found_good_match = True\n",
    "            \n",
    "            if not found_good_match:\n",
    "                print(\"   âŒ NO GOOD MATCHES FOUND!\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during search test: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"ğŸš€ STARTING EMBEDDING CREATION FROM JSONL...\")\n",
    "    \n",
    "    try:\n",
    "        # Setup paths\n",
    "        paths = setup_paths()\n",
    "        print(f\"ğŸ“ Project root: {paths['project_root']}\")\n",
    "        print(f\"ğŸ“ FAISS dir: {paths['faiss_dir']}\")\n",
    "        print(f\"ğŸ“ JSONL source: {paths['jsonl_path']}\")\n",
    "        \n",
    "        # Step 1: Load chunks dari JSONL\n",
    "        print(\"\\nğŸ“– Step 1: Loading chunks from JSONL...\")\n",
    "        chunks = load_chunks_from_jsonl(paths['jsonl_path'])\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"âŒ No chunks loaded from JSONL!\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Create embeddings\n",
    "        print(\"\\nğŸ”¤ Step 2: Creating embeddings...\")\n",
    "        embeddings = create_embeddings(chunks)\n",
    "        \n",
    "        # Step 3: Create FAISS index\n",
    "        print(\"\\nğŸ“Š Step 3: Creating FAISS index...\")\n",
    "        index = create_faiss_index(embeddings)\n",
    "        \n",
    "        # Step 4: Save data\n",
    "        print(\"\\nğŸ’¾ Step 4: Saving data...\")\n",
    "        save_data(chunks, index, paths)\n",
    "        \n",
    "        # Step 5: Verify\n",
    "        print(\"\\nâœ… Step 5: Verification...\")\n",
    "        verify_data(paths)\n",
    "        \n",
    "        # Step 6: Test search\n",
    "        print(\"\\nğŸ” Step 6: Testing search capability...\")\n",
    "        test_search_capability(paths)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ EMBEDDING CREATION COMPLETED!\")\n",
    "        print(f\"ğŸ“Š Total chunks: {len(chunks)}\")\n",
    "        print(f\"ğŸ“Š Embedding dimension: {embeddings.shape[1]}\")\n",
    "        print(f\"ğŸ“Š FAISS index size: {index.ntotal} vectors\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ NEXT STEPS:\")\n",
    "        print(f\"1. Run your Streamlit app\")\n",
    "        print(f\"2. Clear cache di halaman 'ğŸ§¹ Cache'\")\n",
    "        print(f\"3. Test dengan: 'Siapa orang pertama yang divaksin?'\")\n",
    "        print(f\"4. Test dengan: 'Apa vaksin kedua yang digunakan?'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Jalankan langsung jika di-run sebagai script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
