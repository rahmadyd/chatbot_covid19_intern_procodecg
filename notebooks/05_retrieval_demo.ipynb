{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85a76615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS index berhasil dimuat.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "faiss_path = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\"\n",
    "index = faiss.read_index(faiss_path)\n",
    "print(\"âœ… FAISS index berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81307e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama server aktif.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    requests.get(\"http://localhost:11434\", timeout=3)\n",
    "    print(\"âœ… Ollama server aktif.\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"âŒ Ollama belum jalan. Jalankan di terminal: ollama serve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b661763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Adjusted Query:  {\"keywords\": \"jumlah kasus covid bengkulu\"}\n",
      "\n",
      "\n",
      "ðŸ’¬ Jawaban Chatbot:\n",
      "  Jawab: Maaf, untuk informasi tersebut, Anda dapat melakukan pengecekan langsung melalui situs resmi COVID-19 Indonesia yang adalah sumber informasi terpercaya tentang statistik kasus COVID-19 di seluruh wilayah Indonesia. Untuk informasi kasus konkrit di provinsi Bengkulu, Anda dapat melakukan pengecekan secara langsung melalui situs covid19.go.id atau laporan harian Kementerian Kesehatan.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, json\n",
    "\n",
    "# --- Load embedding model ---\n",
    "embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# --- Load FAISS index ---\n",
    "index = faiss.read_index(\"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\")\n",
    "with open(\"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "# --- Prompt sistem chatbot utama ---\n",
    "system_prompt = \"\"\"\n",
    "Anda adalah asisten AI yang ramah, profesional, dan informatif.\n",
    "Tugas Anda adalah merangkum dan menganalisis informasi di blok 'KONTEKS:' untuk menjawab pertanyaan pengguna.\n",
    "\n",
    "ATURAN GAYA & TONE (Untuk Gaya ChatGPT):\n",
    "1. JAWABAN HARUS DISUSUN DALAM PARAGRAF LENGKAP DAN NARATIF.\n",
    "2. Gunakan nada bicara yang ramah, membantu, dan sedikit formal, seperti asisten yang berpengetahuan luas.\n",
    "3. Gunakan Bahasa Indonesia Baku. DILARANG KERAS menggunakan istilah asing.\n",
    "4. Gunakan pemformatan Markdown (seperti **tebal** atau *poin-poin*) untuk menyoroti poin penting jika relevan, tetapi utamakan paragraf naratif.\n",
    "\n",
    "ATURAN KONTEN WAJIB:\n",
    "1. Jawab HANYA dari KONTEKS. DILARANG KERAS menggunakan pengetahuan umum.\n",
    "2. Jika konteks tidak cukup, HANYA katakan: 'Maaf, informasi tidak ditemukan dalam dokumen sumber.'\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 1: Query Adjustment ---\n",
    "def adjust_query(question):\n",
    "    prompt_adjust = \"\"\"\n",
    "    Anda adalah alat ekstraksi kata kunci. Tugas Anda adalah mengubah pertanyaan pengguna menjadi 3 hingga MAKSIMAL 5 KATA KUNCI utama yang relevan untuk pencarian.\n",
    "\n",
    "    ATURAN BAHASA WAJIB: KATA KUNCI HARUS SEPENUHNYA DALAM BAHASA INDONESIA.\n",
    "    \n",
    "    OUTPUT Anda HANYA BOLEH berupa objek JSON.\n",
    "    FORMAT WAJIB JSON: {\"keywords\": \"kasus covid bengkulu 3/31/2020\"} \n",
    "    # HILANGKAN KATA 'TAHUN' jika ada tanggal spesifik.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"mistral:7b-instruct\", messages=[\n",
    "        # Menggunakan System Role untuk instruksi\n",
    "        {\"role\": \"system\", \"content\": prompt_adjust},\n",
    "        # Input pertanyaan\n",
    "        {\"role\": \"user\", \"content\": f\"Pertanyaan: {question}\\n\\nOUTPUT KATA KUNCI:\"},\n",
    "    ])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# --- Step 2: RAG Chat ---\n",
    "def rag_chat(question):\n",
    "    adjusted = adjust_query(question)\n",
    "    print(f\"\\nðŸ”§ Adjusted Query: {adjusted}\\n\")\n",
    "\n",
    "    # Embedding pertanyaan yang sudah di-adjust\n",
    "    q_emb = embedder.encode([adjusted], convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    # Cari dokumen yang paling relevan di FAISS\n",
    "    D, I = index.search(q_emb, k=5)\n",
    "    contexts = \"\\n\".join([texts[i] for i in I[0]])\n",
    "    \n",
    "    # Kueri untuk generate jawaban harus sangat tegas\n",
    "    prompt_generate = f\"\"\"\n",
    "    Gunakan informasi KONTEKS berikut ini untuk menjawab pertanyaan.\n",
    "    JANGAN MENGGUNAKAN PENGETAHUAN UMUM. Jika konteks tidak relevan atau tidak cukup, katakan 'Maaf, informasi tidak ditemukan dalam dokumen sumber.'\n",
    "\n",
    "    KONTEKS:\n",
    "    ---\n",
    "    {contexts}\n",
    "    ---\n",
    "\n",
    "    PERTANYAAN: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate jawaban dari hasil retrieval\n",
    "    response = ollama.chat(model=\"mistral:7b-instruct\", messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_generate} # User content hanya berisi prompt generation yang sudah digabung\n",
    "    ])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "user_query = \"Berapa jumlah kasus covid di bengkulu?\"\n",
    "\n",
    "# Jalankan RAG\n",
    "answer = rag_chat(user_query)\n",
    "print(\"\\nðŸ’¬ Jawaban Chatbot:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d688c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS Index berhasil dimuat. Dimensi: 768, Total Vektor: 20\n",
      "\n",
      "ðŸ”§ Adjusted Query: gejala sesak napas perlu dokter\n",
      "\n",
      "\n",
      "ðŸ’¬ Jawaban Chatbot:\n",
      "  Jawabannya, gejala sesak napas dapat menunjukkan beberapa masalah medis yang serius seperti infeksi saluran paru-paru atau penyakit jantung. Selain itu, gejala sesak napas juga bisa merupakan tanda dari kegagalan tubuh untuk mengambil udara secara normal. Untuk memastikan masalah ini terdiagnosis dan dipulihkan segera, konsultasi dengan dokter wajib dilakukan.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- PENTING: PASTIKAN INDEX FAISS SUDAH DI-REBUILD DENGAN MPNET V2 (Dimensi 768) ---\n",
    "\n",
    "# --- Load embedding model (MPNet V2 - Cerdas & Multibahasa) ---\n",
    "embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# --- Load FAISS index ---\n",
    "# Pastikan ini adalah file index yang sudah di-rebuild!\n",
    "INDEX_PATH = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\"\n",
    "TEXTS_PATH = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19_texts.json\"\n",
    "\n",
    "try:\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(TEXTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = json.load(f)\n",
    "    print(f\"âœ… FAISS Index berhasil dimuat. Dimensi: {index.d}, Total Vektor: {index.ntotal}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GAGAL memuat FAISS Index: {e}\")\n",
    "    print(\"Mohon pastikan Anda sudah me-REBUILD index menggunakan model 'paraphrase-multilingual-mpnet-base-v2'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Model LLM yang Cerdas dan Patuh ---\n",
    "LLM_MODEL = \"mistral:7b-instruct\"\n",
    "\n",
    "# --- Prompt Sistem Chatbot Utama (Tahap 2: Generation) ---\n",
    "system_prompt = \"\"\"\n",
    "Anda adalah asisten AI yang ramah, profesional, dan informatif, ahli dalam data COVID-19 Indonesia.\n",
    "Tugas Anda adalah merangkum dan menganalisis informasi yang diberikan di blok 'KONTEKS:' untuk menjawab pertanyaan pengguna.\n",
    "\n",
    "ATURAN GAYA & TONE (Untuk Gaya ChatGPT):\n",
    "1. JAWABAN HARUS DISUSUN DALAM PARAGRAF LENGKAP DAN NARATIF.\n",
    "2. Gunakan nada bicara yang ramah, profesional, dan sedikit formal.\n",
    "3. Gunakan **Bahasa Indonesia Baku**. DILARANG KERAS menggunakan istilah asing (seperti 'cases' atau 'recovery').\n",
    "4. Jika memungkinkan, gunakan pemformatan Markdown (**tebal**) untuk menyoroti angka atau nama provinsi.\n",
    "\n",
    "ATURAN FILTER KONTEKS KRITIS:\n",
    "1. Jawab HANYA dari KONTEKS dan JAWABAN HARUS SANGAT SPESIFIK PADA WILAYAH YANG DITANYAKAN (misalnya, Bengkulu).\n",
    "2. Jika KONTEKS yang diambil oleh sistem mengandung data provinsi lain (misalnya Lampung, Jawa Tengah) yang BUKAN Bengkulu, **ABAIKAN SEMUA DATA PROVINSI LAIN TERSEBUT**.\n",
    "\n",
    "ATURAN KONTEN WAJIB:\n",
    "1. JANGAN PERNAH menggunakan pengetahuan umum.\n",
    "2. Jika hanya data provinsi yang SALAH yang muncul di KONTEKS, atau jika tidak ada data yang relevan sama sekali, HANYA katakan: 'Maaf, informasi tidak ditemukan dalam dokumen sumber.'\n",
    "3. Jika pertanyaan umum (tanpa tanggal spesifik) muncul, ambil data TERBARU atau angka TOTAL yang tersedia dalam KONTEKS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Step 1: Query Adjustment (Pemaksaan JSON dan Bahasa Indonesia) ---\n",
    "def adjust_query(question):\n",
    "    prompt_adjust = \"\"\"\n",
    "    Tugas: Ubah pertanyaan pengguna menjadi FRASA KATA KUNCI SEMANTIK SANGAT RINGKAS (MAKSIMAL 5 KATA KUNCI).\n",
    "\n",
    "    ATURAN BAHASA WAJIB: KATA KUNCI HARUS SEPENUHNYA DALAM BAHASA INDONESIA.\n",
    "    \n",
    "    OUTPUT Anda HANYA BOLEH berupa objek JSON.\n",
    "    FORMAT WAJIB JSON: {\"keywords\": \"jumlah total kasus covid bengkulu\"} \n",
    "    DILARANG KERAS membuat narasi, penjelasan, atau teks di luar format JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=LLM_MODEL, messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_adjust},\n",
    "        {\"role\": \"user\", \"content\": f\"Pertanyaan: {question}\\n\\nOUTPUT KATA KUNCI:\"},\n",
    "    ])\n",
    "    \n",
    "    output_text = response[\"message\"][\"content\"].strip()\n",
    "    \n",
    "    # 1. Parsing JSON yang aman\n",
    "    try:\n",
    "        # Hapus prefix/suffix yang mungkin ditambahkan LLM (seperti ```json)\n",
    "        json_string = re.search(r'\\{.*\\}', output_text, re.DOTALL).group(0)\n",
    "        parsed_json = json.loads(json_string)\n",
    "        keywords = parsed_json.get(\"keywords\", question)\n",
    "        \n",
    "        # 2. Pembersihan Akhir: Hapus tanda baca sisa\n",
    "        cleaned_keywords = keywords.replace(',', '').replace('?', '').replace('.', '').strip()\n",
    "        \n",
    "        # 3. Handle jika LLM memasukkan kata yang tidak perlu (seperti \"keywords:\")\n",
    "        if cleaned_keywords.lower().startswith(\"keywords:\"):\n",
    "            cleaned_keywords = cleaned_keywords[9:].strip()\n",
    "            \n",
    "        return cleaned_keywords\n",
    "        \n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        # Fallback jika parsing JSON gagal\n",
    "        print(\"âš ï¸ Warning: LLM gagal memproduksi JSON. Menggunakan kueri asli.\")\n",
    "        return question.replace(',', ' ')\n",
    "\n",
    "\n",
    "# --- Step 2: RAG Chat ---\n",
    "def rag_chat(question):\n",
    "    # Dapatkan kueri yang sudah di-adjust\n",
    "    adjusted = adjust_query(question)\n",
    "    print(f\"\\nðŸ”§ Adjusted Query: {adjusted}\\n\")\n",
    "\n",
    "    # Embedding pertanyaan yang sudah di-adjust\n",
    "    q_emb = embedder.encode([adjusted], convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    # Cari dokumen yang paling relevan di FAISS (k=5 untuk indeks gabungan)\n",
    "    K_VALUE = 20 \n",
    "    D, I = index.search(q_emb, k=K_VALUE)\n",
    "    \n",
    "    # Kumpulkan konteks\n",
    "    contexts = \"\\n\".join([texts[i] for i in I[0]])\n",
    "    \n",
    "    # Kueri untuk generate jawaban\n",
    "    prompt_generate = f\"\"\"\n",
    "    KONTEKS:\n",
    "    ---\n",
    "    {contexts}\n",
    "    ---\n",
    "    \n",
    "    PERTANYAAN: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate jawaban\n",
    "    response = ollama.chat(model=LLM_MODEL, messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_generate}\n",
    "    ])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# --- EKSEKUSI (Uji dengan Kueri yang Relevan dengan Data) ---\n",
    "user_query = \"Kenapa kita harus ke dokter setelah ada gejala sesak napas?\"\n",
    "\n",
    "# Jalankan RAG\n",
    "answer = rag_chat(user_query)\n",
    "print(\"\\nðŸ’¬ Jawaban Chatbot:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7886bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source_info(text):\n",
    "    \"\"\"Contoh untuk data COVID-19 dengan format spesifik\"\"\"\n",
    "    # Jika data memiliki format tanggal\n",
    "    date_pattern = r\"(\\d{1,2}/\\d{1,2}/\\d{4})\"\n",
    "    date_match = re.search(date_pattern, text)\n",
    "    \n",
    "    # Jika data memiliki format provinsi\n",
    "    province_pattern = r\"(Provinsi\\s+[^,\\n]+)\"\n",
    "    province_match = re.search(province_pattern, text)\n",
    "    \n",
    "    source_parts = []\n",
    "    if province_match:\n",
    "        source_parts.append(province_match.group(1))\n",
    "    if date_match:\n",
    "        source_parts.append(f\"Tanggal: {date_match.group(1)}\")\n",
    "    \n",
    "    if source_parts:\n",
    "        return \" | \".join(source_parts)\n",
    "    \n",
    "    return \"Data COVID-19 Nasional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a398e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot_covid19_intern_procodecg\\.venvr\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS Index berhasil dimuat. Dimensi: 768, Total Vektor: 20\n",
      "======================================================================\n",
      "ðŸ¤– CHATBOT COVID-19 INDONESIA\n",
      "ðŸ“š Dilengkapi dengan Sistem Referensi Dokumen\n",
      "======================================================================\n",
      "\n",
      "Selamat datang! Silakan ajukan pertanyaan tentang COVID-19 di Indonesia.\n",
      "Contoh pertanyaan:\n",
      "  - 'Berapa total kasus COVID-19 di Jawa Barat?'\n",
      "  - 'Apa saja gejala COVID-19 yang umum?'\n",
      "  - 'Bagaimana perkembangan vaksinasi di Bali?'\n",
      "  - 'Ketik 'quit' untuk keluar\n",
      "\n",
      "ðŸ”„ Mencari informasi...\n",
      "\n",
      "ðŸ”§ Adjusted Query: infodemi pandemi\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¡ **JAWABAN:**\n",
      "======================================================================\n",
      " Infodemi dalam konteks pandemi COVID-19 merupakan banjir informasi palsu dan hoaks di media sosial. Saat pandemi tersebut, teori konspirasi seperti vaksin, obat herbal, dan microchip sempat menyebar luas, yang membahayakan kepercayaan masyarakat terhadap informasi yang benar. Untuk mengatasi infodemi ini, pemerintah melakukan klarifikasi fakta dan kampanye literasi digital serta menjadikan komunikasi publik lebih strategis.\n",
      "\n",
      "---\n",
      "**ðŸ“š Sumber Referensi:**\n",
      "\n",
      "**1. Dokumen #1: 2. Program Vaksinasi Nasional Pemerintah Indonesia memulai program...**\n",
      "   - **Tingkat Kepercayaan:** ðŸŸ¢ Tinggi\n",
      "   - **Skor Relevansi:** 7.329\n",
      "   - **Cuplikan:** 2. Program Vaksinasi Nasional\n",
      "Pemerintah Indonesia memulai program vaksinasi nasional pada Januari 2021 sebagai upaya me...\n",
      "\n",
      "**2. Dokumen #0: 1. Informasi Medis dan Kesehatan Publik COVID-19 adalah...**\n",
      "   - **Tingkat Kepercayaan:** ðŸŸ¢ Tinggi\n",
      "   - **Skor Relevansi:** 6.930\n",
      "   - **Cuplikan:** 1. Informasi Medis dan Kesehatan Publik\n",
      "COVID-19 adalah penyakit menular yang disebabkan oleh virus SARS-CoV-2. Penulara...\n",
      "\n",
      "**3. Dokumen #13: 14. Ekonomi Nasional dan UMKM Pandemi membawa dampak...**\n",
      "   - **Tingkat Kepercayaan:** ðŸŸ¢ Tinggi\n",
      "   - **Skor Relevansi:** 6.853\n",
      "   - **Cuplikan:** 14. Ekonomi Nasional dan UMKM\n",
      "Pandemi membawa dampak ekonomi besar di Indonesia. Pertumbuhan ekonomi 2020 terkontraksi h...\n",
      "\n",
      "**4. Dokumen #14: 15. Dampak Sosial dan Budaya Pandemi mengubah kehidupan...**\n",
      "   - **Tingkat Kepercayaan:** ðŸŸ¢ Tinggi\n",
      "   - **Skor Relevansi:** 6.785\n",
      "   - **Cuplikan:** 15. Dampak Sosial dan Budaya\n",
      "Pandemi mengubah kehidupan sosial masyarakat Indonesia secara mendalam. Aktivitas keagamaan...\n",
      "\n",
      "---\n",
      "*ðŸ” Berdasarkan analisis 15 dokumen | Kueri: 'infodemi pandemi' | Tanggal: 24/10/2025 15:22*\n",
      "\n",
      "\n",
      "ðŸ” **INFORMASI DEBUG:**\n",
      "ðŸ“Š Jumlah dokumen yang ditemukan: 15\n",
      "ðŸŽ¯ Dokumen terbaik yang digunakan: 15\n",
      "ðŸ”§ Kueri yang digunakan: 'infodemi pandemi'\n",
      "\n",
      "ðŸ“ˆ Top 5 dokumen terbaik:\n",
      "  ðŸŸ¢ 1. Skor: 3.8349 | Dokumen #15: 16. Komunikasi Publik dan Infodemi Selama pandemi, Indonesia...\n",
      "  ðŸŸ¢ 2. Skor: 3.9645 | data kesehatan nasional, dan pembangunan pusat ris... | Dokumen #18\n",
      "  ðŸŸ¢ 3. Skor: 4.1556 | Data Nasional\n",
      "Selama pandemi, Indonesia secara rut... | Dokumen #2\n",
      "  ðŸŸ¢ 4. Skor: 4.6936 | Dokumen #3: 4. Panduan Aktivitas dan Mobilitas Untuk menekan penyebaran...\n",
      "  ðŸŸ¢ 5. Skor: 4.6986 | Dokumen #11: 12. Sistem Kesehatan Nasional Sistem kesehatan Indonesia menghadapi...\n",
      "\n",
      "ðŸ“Š Statistik Skor: Rata-rata: 5.5062 | Maks: 7.3294 | Min: 3.8349\n",
      "\n",
      "======================================================================\n",
      "âš ï¸ Silakan masukkan pertanyaan.\n",
      "âš ï¸ Silakan masukkan pertanyaan.\n",
      "âš ï¸ Silakan masukkan pertanyaan.\n",
      "âš ï¸ Silakan masukkan pertanyaan.\n",
      "ðŸ”„ Mencari informasi...\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Load embedding model (MPNet V2 - Cerdas & Multibahasa) ---\n",
    "embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# --- Load FAISS index ---\n",
    "INDEX_PATH = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19.index\"\n",
    "TEXTS_PATH = \"D:/chatbot_covid19_intern_procodecg/faiss/faiss_textcovid19_texts.json\"\n",
    "\n",
    "try:\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(TEXTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = json.load(f)\n",
    "    print(f\"âœ… FAISS Index berhasil dimuat. Dimensi: {index.d}, Total Vektor: {index.ntotal}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GAGAL memuat FAISS Index: {e}\")\n",
    "    print(\"Mohon pastikan Anda sudah me-REBUILD index menggunakan model 'paraphrase-multilingual-mpnet-base-v2'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Model LLM yang Cerdas dan Patuh ---\n",
    "LLM_MODEL = \"mistral:7b-instruct\"\n",
    "\n",
    "# --- Prompt Sistem Chatbot Utama (Tahap 2: Generation) ---\n",
    "system_prompt = \"\"\"\n",
    "Anda adalah asisten AI yang ramah, profesional, dan informatif, ahli dalam data COVID-19 Indonesia.\n",
    "Tugas Anda adalah merangkum dan menganalisis informasi yang diberikan di blok 'KONTEKS:' untuk menjawab pertanyaan pengguna.\n",
    "\n",
    "ATURAN GAYA & TONE (Untuk Gaya ChatGPT):\n",
    "1. JAWABAN HARUS DISUSUN DALAM PARAGRAF LENGKAP DAN NARATIF.\n",
    "2. Gunakan nada bicara yang ramah, profesional, dan sedikit formal.\n",
    "3. Gunakan **Bahasa Indonesia Baku**. DILARANG KERAS menggunakan istilah asing (seperti 'cases' atau 'recovery').\n",
    "4. Jika memungkinkan, gunakan pemformatan Markdown (**tebal**) untuk menyoroti angka atau nama provinsi.\n",
    "\n",
    "ATURAN FILTER KONTEKS KRITIS:\n",
    "1. Jawab HANYA dari KONTEKS dan JAWABAN HARUS SANGAT SPESIFIK PADA WILAYAH YANG DITANYAKAN (misalnya, Bengkulu).\n",
    "2. Jika KONTEKS yang diambil oleh sistem mengandung data provinsi lain (misalnya Lampung, Jawa Tengah) yang BUKAN Bengkulu, **ABAIKAN SEMUA DATA PROVINSI LAIN TERSEBUT**.\n",
    "\n",
    "ATURAN KONTEN WAJIB:\n",
    "1. JANGAN PERNAH menggunakan pengetahuan umum.\n",
    "2. Jika hanya data provinsi yang SALAH yang muncul di KONTEKS, atau jika tidak ada data yang relevan sama sekali, HANYA katakan: 'Maaf, informasi tidak ditemukan dalam dokumen sumber.'\n",
    "3. Jika pertanyaan umum (tanpa tanggal spesifik) muncul, ambil data TERBARU atau angka TOTAL yang tersedia dalam KONTEKS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 1: Query Adjustment (Pemaksaan JSON dan Bahasa Indonesia) ---\n",
    "def adjust_query(question):\n",
    "    prompt_adjust = \"\"\"\n",
    "    Tugas: Ubah pertanyaan pengguna menjadi FRASA KATA KUNCI SEMANTIK SANGAT RINGKAS (MAKSIMAL 5 KATA KUNCI).\n",
    "\n",
    "    ATURAN BAHASA WAJIB: KATA KUNCI HARUS SEPENUHNYA DALAM BAHASA INDONESIA.\n",
    "    \n",
    "    OUTPUT Anda HANYA BOLEH berupa objek JSON.\n",
    "    FORMAT WAJIB JSON: {\"keywords\": \"jumlah total kasus covid bengkulu\"} \n",
    "    DILARANG KERAS membuat narasi, penjelasan, atau teks di luar format JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=LLM_MODEL, messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_adjust},\n",
    "        {\"role\": \"user\", \"content\": f\"Pertanyaan: {question}\\n\\nOUTPUT KATA KUNCI:\"},\n",
    "    ])\n",
    "    \n",
    "    output_text = response[\"message\"][\"content\"].strip()\n",
    "    \n",
    "    # 1. Parsing JSON yang aman\n",
    "    try:\n",
    "        # Hapus prefix/suffix yang mungkin ditambahkan LLM (seperti ```json)\n",
    "        json_string = re.search(r'\\{.*\\}', output_text, re.DOTALL).group(0)\n",
    "        parsed_json = json.loads(json_string)\n",
    "        keywords = parsed_json.get(\"keywords\", question)\n",
    "        \n",
    "        # 2. Pembersihan Akhir: Hapus tanda baca sisa\n",
    "        cleaned_keywords = keywords.replace(',', '').replace('?', '').replace('.', '').strip()\n",
    "        \n",
    "        # 3. Handle jika LLM memasukkan kata yang tidak perlu (seperti \"keywords:\")\n",
    "        if cleaned_keywords.lower().startswith(\"keywords:\"):\n",
    "            cleaned_keywords = cleaned_keywords[9:].strip()\n",
    "            \n",
    "        return cleaned_keywords\n",
    "        \n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        # Fallback jika parsing JSON gagal\n",
    "        print(\"âš ï¸ Warning: LLM gagal memproduksi JSON. Menggunakan kueri asli.\")\n",
    "        return question.replace(',', ' ')\n",
    "\n",
    "# --- Fungsi untuk mengekstrak informasi sumber dari teks ---\n",
    "def extract_source_info(text, index_position):\n",
    "    \"\"\"\n",
    "    Mengekstrak informasi sumber dari teks dokumen.\n",
    "    Menyesuaikan dengan berbagai format data COVID-19.\n",
    "    \"\"\"\n",
    "    # Pattern untuk provinsi\n",
    "    province_pattern = r\"(Provinsi\\s+[^,\\n\\.]+|provinsi\\s+[^,\\n\\.]+)\"\n",
    "    province_match = re.search(province_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Pattern untuk tanggal\n",
    "    date_pattern = r\"(\\d{1,2}/\\d{1,2}/\\d{4}|\\d{1,2}-\\d{1,2}-\\d{4}|\\d{4}-\\d{1,2}-\\d{1,2})\"\n",
    "    date_match = re.search(date_pattern, text)\n",
    "    \n",
    "    # Pattern untuk laporan/resmi\n",
    "    report_pattern = r\"(Laporan\\s+[^\\.]+?\\.|Data\\s+[^\\.]+?\\.|Update\\s+[^\\.]+?\\.|Situasi\\s+[^\\.]+?\\.)\"\n",
    "    report_match = re.search(report_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Pattern untuk sumber eksplisit\n",
    "    source_pattern = r\"(Sumber:\\s*[^\\n]+|Berdasarkan\\s+[^\\.]+?\\.|Menurut\\s+[^\\.]+?\\.)\"\n",
    "    source_match = re.search(source_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Pattern untuk angka kasus (sebagai konteks)\n",
    "    case_pattern = r\"(kasus\\s+(?:terkonfirmasi|positif|aktif|sembuh|meninggal)[^\\.]*\\d+)\"\n",
    "    case_match = re.search(case_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Bangun informasi sumber\n",
    "    source_parts = []\n",
    "    \n",
    "    if province_match:\n",
    "        source_parts.append(province_match.group(1).title())\n",
    "    \n",
    "    if date_match:\n",
    "        source_parts.append(f\"Tanggal: {date_match.group(1)}\")\n",
    "    \n",
    "    if report_match:\n",
    "        report_text = report_match.group(1)\n",
    "        if len(report_text) > 50:\n",
    "            report_text = report_text[:50] + \"...\"\n",
    "        source_parts.append(report_text)\n",
    "    elif source_match:\n",
    "        source_text = source_match.group(1)\n",
    "        if len(source_text) > 50:\n",
    "            source_text = source_text[:50] + \"...\"\n",
    "        source_parts.append(source_text)\n",
    "    elif case_match:\n",
    "        source_parts.append(\"Data Kasus COVID-19\")\n",
    "    \n",
    "    # Jika tidak ada pola yang ditemukan, buat sumber generik\n",
    "    if not source_parts:\n",
    "        # Ambil beberapa kata pertama sebagai konteks\n",
    "        words = text.split()[:8]\n",
    "        preview = \" \".join(words) + \"...\"\n",
    "        source_parts.append(f\"Dokumen #{index_position}: {preview}\")\n",
    "    else:\n",
    "        source_parts.append(f\"Dokumen #{index_position}\")\n",
    "    \n",
    "    return \" | \".join(source_parts)\n",
    "\n",
    "# --- Step 2: RAG Chat dengan Sumber Referensi ---\n",
    "def rag_chat(question):\n",
    "    # Dapatkan kueri yang sudah di-adjust\n",
    "    adjusted = adjust_query(question)\n",
    "    print(f\"\\nðŸ”§ Adjusted Query: {adjusted}\\n\")\n",
    "\n",
    "    # Embedding pertanyaan yang sudah di-adjust\n",
    "    q_emb = embedder.encode([adjusted], convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    # Cari dokumen yang paling relevan di FAISS\n",
    "    K_VALUE = 15\n",
    "    D, I = index.search(q_emb, k=K_VALUE)\n",
    "    \n",
    "    # Kumpulkan konteks dan simpan informasi sumber\n",
    "    contexts = []\n",
    "    source_info = []\n",
    "    \n",
    "    for i, (idx, score) in enumerate(zip(I[0], D[0])):\n",
    "        if idx < len(texts):  # Pastikan index valid\n",
    "            context_text = texts[idx]\n",
    "            contexts.append(context_text)\n",
    "            \n",
    "            # Ekstrak informasi sumber\n",
    "            source_line = extract_source_info(context_text, idx)\n",
    "            source_info.append({\n",
    "                \"rank\": i + 1,\n",
    "                \"similarity_score\": float(score),\n",
    "                \"text_preview\": context_text[:120] + \"...\" if len(context_text) > 120 else context_text,\n",
    "                \"source\": source_line,\n",
    "                \"document_id\": int(idx)\n",
    "            })\n",
    "    \n",
    "    # Gabungkan konteks untuk LLM\n",
    "    contexts_text = \"\\n---\\n\".join(contexts[:8])  # Gunakan 8 dokumen terbaik untuk konteks\n",
    "    \n",
    "    # Kueri untuk generate jawaban\n",
    "    prompt_generate = f\"\"\"\n",
    "    KONTEKS:\n",
    "    {contexts_text}\n",
    "\n",
    "    PERTANYAAN: {question}\n",
    "\n",
    "    INSTRUKSI: Jawablah pertanyaan di atas HANYA berdasarkan informasi yang ada dalam KONTEKS di atas. \n",
    "    Jika informasi tidak cukup atau tidak relevan, katakan bahwa informasi tidak ditemukan.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate jawaban\n",
    "    try:\n",
    "        response = ollama.chat(model=LLM_MODEL, messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_generate}\n",
    "        ])\n",
    "        answer = response[\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        answer = f\"Maaf, terjadi kesalahan dalam menghasilkan jawaban: {str(e)}\"\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": source_info,\n",
    "        \"adjusted_query\": adjusted,\n",
    "        \"contexts_used\": len(contexts)\n",
    "    }\n",
    "\n",
    "# --- Fungsi untuk memformat output dengan referensi ---\n",
    "def format_response(response_data, show_detailed_sources=True):\n",
    "    \"\"\"Memformat response dengan jawaban dan referensi\"\"\"\n",
    "    answer = response_data[\"answer\"]\n",
    "    sources = response_data[\"sources\"]\n",
    "    adjusted_query = response_data[\"adjusted_query\"]\n",
    "    contexts_used = response_data[\"contexts_used\"]\n",
    "    \n",
    "    # Filter sumber dengan similarity score tinggi\n",
    "    high_confidence_sources = [s for s in sources if s[\"similarity_score\"] > 0.4]\n",
    "    \n",
    "    formatted_response = f\"{answer}\\n\\n\"\n",
    "    formatted_response += \"---\\n\"\n",
    "    formatted_response += \"**ðŸ“š Sumber Referensi:**\\n\\n\"\n",
    "    \n",
    "    if high_confidence_sources:\n",
    "        # Urutkan berdasarkan similarity score (descending)\n",
    "        high_confidence_sources.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "        \n",
    "        for i, source in enumerate(high_confidence_sources[:4]):  # Tampilkan max 4 referensi terbaik\n",
    "            confidence_level = \"ðŸŸ¢ Tinggi\" if source[\"similarity_score\"] > 0.7 else \"ðŸŸ¡ Sedang\" if source[\"similarity_score\"] > 0.5 else \"ðŸ”µ Rendah\"\n",
    "            \n",
    "            formatted_response += f\"**{i+1}. {source['source']}**\\n\"\n",
    "            formatted_response += f\"   - **Tingkat Kepercayaan:** {confidence_level}\\n\"\n",
    "            formatted_response += f\"   - **Skor Relevansi:** {source['similarity_score']:.3f}\\n\"\n",
    "            \n",
    "            if show_detailed_sources:\n",
    "                formatted_response += f\"   - **Cuplikan:** {source['text_preview']}\\n\"\n",
    "            \n",
    "            formatted_response += \"\\n\"\n",
    "    else:\n",
    "        formatted_response += \"Informasi dikumpulkan dari berbagai dokumen COVID-19 terpercaya.\\n\\n\"\n",
    "    \n",
    "    # Footer informasi\n",
    "    formatted_response += \"---\\n\"\n",
    "    formatted_response += f\"*ðŸ” Berdasarkan analisis {contexts_used} dokumen | \"\n",
    "    formatted_response += f\"Kueri: '{adjusted_query}' | \"\n",
    "    formatted_response += f\"Tanggal: {datetime.now().strftime('%d/%m/%Y %H:%M')}*\\n\"\n",
    "    \n",
    "    return formatted_response\n",
    "\n",
    "# --- Fungsi untuk menampilkan informasi debug ---\n",
    "def show_debug_info(response_data):\n",
    "    \"\"\"Menampilkan informasi debug untuk developer\"\"\"\n",
    "    sources = response_data[\"sources\"]\n",
    "    \n",
    "    print(f\"\\nðŸ” **INFORMASI DEBUG:**\")\n",
    "    print(f\"ðŸ“Š Jumlah dokumen yang ditemukan: {len(sources)}\")\n",
    "    print(f\"ðŸŽ¯ Dokumen terbaik yang digunakan: {response_data['contexts_used']}\")\n",
    "    print(f\"ðŸ”§ Kueri yang digunakan: '{response_data['adjusted_query']}'\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Top 5 dokumen terbaik:\")\n",
    "    for i, source in enumerate(sources[:5]):\n",
    "        confidence_icon = \"ðŸŸ¢\" if source[\"similarity_score\"] > 0.7 else \"ðŸŸ¡\" if source[\"similarity_score\"] > 0.5 else \"ðŸ”µ\"\n",
    "        print(f\"  {confidence_icon} {i+1}. Skor: {source['similarity_score']:.4f} | {source['source']}\")\n",
    "    \n",
    "    # Statistik skor\n",
    "    if sources:\n",
    "        avg_score = sum(s[\"similarity_score\"] for s in sources) / len(sources)\n",
    "        max_score = max(s[\"similarity_score\"] for s in sources)\n",
    "        min_score = min(s[\"similarity_score\"] for s in sources)\n",
    "        print(f\"\\nðŸ“Š Statistik Skor: Rata-rata: {avg_score:.4f} | Maks: {max_score:.4f} | Min: {min_score:.4f}\")\n",
    "\n",
    "# --- Fungsi utama untuk chat interaktif ---\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ¤– CHATBOT COVID-19 INDONESIA\")\n",
    "    print(\"ðŸ“š Dilengkapi dengan Sistem Referensi Dokumen\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nSelamat datang! Silakan ajukan pertanyaan tentang COVID-19 di Indonesia.\")\n",
    "    print(\"Contoh pertanyaan:\")\n",
    "    print(\"  - 'Berapa total kasus COVID-19 di Jawa Barat?'\")\n",
    "    print(\"  - 'Apa saja gejala COVID-19 yang umum?'\")\n",
    "    print(\"  - 'Bagaimana perkembangan vaksinasi di Bali?'\")\n",
    "    print(\"  - 'Ketik 'quit' untuk keluar\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\nðŸ’¬ Pertanyaan Anda: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'keluar', 'q']:\n",
    "                print(\"Terima kasih telah menggunakan Chatbot COVID-19!\")\n",
    "                break\n",
    "            \n",
    "            if not user_query:\n",
    "                print(\"âš ï¸ Silakan masukkan pertanyaan.\")\n",
    "                continue\n",
    "            \n",
    "            print(\"ðŸ”„ Mencari informasi...\")\n",
    "            \n",
    "            # Jalankan RAG\n",
    "            response_data = rag_chat(user_query)\n",
    "            \n",
    "            # Format dan tampilkan response\n",
    "            formatted_answer = format_response(response_data)\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ’¡ **JAWABAN:**\")\n",
    "            print(\"=\" * 70)\n",
    "            print(formatted_answer)\n",
    "            \n",
    "            # Tampilkan info debug (opsional)\n",
    "            debug_choice = input(\"\\nðŸ”§ Tampilkan informasi detail? (y/n): \").lower()\n",
    "            if debug_choice == 'y':\n",
    "                show_debug_info(response_data)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nTerima kasih telah menggunakan Chatbot COVID-19!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Terjadi kesalahan: {str(e)}\")\n",
    "            print(\"Silakan coba lagi dengan pertanyaan yang berbeda.\")\n",
    "\n",
    "# --- Mode testing cepat ---\n",
    "def quick_test():\n",
    "    \"\"\"Mode testing untuk menguji dengan pertanyaan cepat\"\"\"\n",
    "    test_questions = [\n",
    "        \"Berapa total kasus COVID-19 di Jakarta?\",\n",
    "        \"Apa saja gejala umum COVID-19?\",\n",
    "        \"Bagaimana perkembangan vaksinasi di Jawa Barat?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸš€ MODE TESTING CEPAT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Testing: '{question}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        response_data = rag_chat(question)\n",
    "        formatted_answer = format_response(response_data, show_detailed_sources=False)\n",
    "        print(formatted_answer)\n",
    "        \n",
    "        # Tampilkan info singkat\n",
    "        top_sources = [s for s in response_data[\"sources\"] if s[\"similarity_score\"] > 0.5][:2]\n",
    "        if top_sources:\n",
    "            print(\"ðŸ“Œ Sumber utama:\")\n",
    "            for source in top_sources:\n",
    "                print(f\"   - {source['source']} (skor: {source['similarity_score']:.3f})\")\n",
    "        \n",
    "        if i < len(test_questions):\n",
    "            input(\"\\nâŽ Press Enter untuk test berikutnya...\")\n",
    "\n",
    "# --- EKSEKUSI PROGRAM ---\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"test\":\n",
    "        quick_test()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
